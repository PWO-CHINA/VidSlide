"""
å½±å¹»æ™ºæ (VidSlide) - è§†é¢‘æå–æ ¸å¿ƒæ¨¡å—
======================================
è´Ÿè´£ä»è§†é¢‘ä¸­æ£€æµ‹åœºæ™¯å˜åŒ–å¹¶æå–å¹»ç¯ç‰‡æˆªå›¾ã€‚
æ”¯æŒ GPU ç¡¬ä»¶åŠ é€Ÿè§£ç ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰å’Œè¿›ç¨‹ä¼˜å…ˆçº§è°ƒæ•´ã€‚

ä½œè€…: PWO-CHINA
ç‰ˆæœ¬: v0.6.1
"""

import cv2
import gc
import numpy as np
import os
import time
import traceback
from concurrent.futures import ThreadPoolExecutor

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

try:
    import av
    HAS_PYAV = True
except ImportError:
    HAS_PYAV = False


# â”€â”€ GPU ç¡¬ä»¶åŠ é€Ÿæ¢æµ‹ï¼ˆåº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡ï¼Œç»“æœç¼“å­˜ï¼‰ â”€â”€
_gpu_probe_cache = None


def probe_gpu():
    """
    æ¢æµ‹ç³»ç»Ÿ GPU ç¡¬ä»¶åŠ é€Ÿèƒ½åŠ›ã€‚ç»“æœå…¨å±€ç¼“å­˜ï¼Œåç»­è°ƒç”¨ç›´æ¥è¿”å›ã€‚
    è¿”å› dict: gpus, pyav, hw_decoders, best_per_codec, summary
    """
    global _gpu_probe_cache
    if _gpu_probe_cache is not None:
        return _gpu_probe_cache

    result = {
        'gpus': [],
        'pyav': HAS_PYAV,
        'hw_decoders': {},       # codec -> [å¯ç”¨ hw_type]
        'best_per_codec': {},    # codec -> æœ€ä¼˜ hw_typeï¼ˆé¦–é€‰ï¼‰
        'summary': ''
    }

    # 1. æ£€æµ‹ GPU è®¾å¤‡åç§°ï¼ˆWindows wmicï¼‰
    if os.name == 'nt':
        try:
            import subprocess
            output = subprocess.check_output(
                ['wmic', 'path', 'win32_VideoController', 'get', 'name'],
                text=True, timeout=5,
                creationflags=0x08000000  # CREATE_NO_WINDOW
            )
            for line in output.strip().split('\n')[1:]:
                name = line.strip()
                if name and name != 'Name':
                    result['gpus'].append(name)
        except Exception:
            pass

    # 2. æ¢æµ‹ PyAV ç¡¬ä»¶åŠ é€Ÿæ”¯æŒ
    if HAS_PYAV:
        try:
            from av.codec.hwaccel import HWAccel
            _hw_order = ('cuda', 'd3d11va', 'qsv', 'dxva2')
            for codec in ('h264', 'hevc', 'av1'):
                available = []
                for hw_type in _hw_order:
                    try:
                        HWAccel(codec=codec, device_type=hw_type)
                        available.append(hw_type)
                    except Exception:
                        pass
                result['hw_decoders'][codec] = available
                if available:
                    result['best_per_codec'][codec] = available[0]
        except ImportError:
            pass

    # 3. ç”Ÿæˆäººç±»å¯è¯»æ‘˜è¦
    gpu_name = result['gpus'][0] if result['gpus'] else 'æœªæ£€æµ‹åˆ° GPU'
    # è¿‡æ»¤è™šæ‹Ÿæ˜¾ç¤ºé€‚é…å™¨
    for g in result['gpus']:
        if 'virtual' not in g.lower() and 'basic' not in g.lower():
            gpu_name = g
            break

    if result['hw_decoders']:
        hw_parts = []
        for codec in ('h264', 'hevc', 'av1'):
            types = result['hw_decoders'].get(codec, [])
            if types:
                hw_parts.append(f"{codec.upper()}: {'/'.join(types)}")
            else:
                label = 'dav1d' if (codec == 'av1' and HAS_PYAV) else 'CPU'
                hw_parts.append(f"{codec.upper()}: {label}")
        result['summary'] = f"{gpu_name} | {', '.join(hw_parts)}"
    else:
        result['summary'] = f"{gpu_name} | PyAV {'å¯ç”¨' if HAS_PYAV else 'æœªå®‰è£…'}"

    _gpu_probe_cache = result
    print(f'[GPU æ¢æµ‹] {result["summary"]}')
    return result


def _lower_process_priority():
    """é™ä½å½“å‰è¿›ç¨‹ä¼˜å…ˆçº§ï¼Œé˜²æ­¢æå–ä»»åŠ¡æŠ¢å ç³»ç»Ÿèµ„æº"""
    if not HAS_PSUTIL:
        return
    try:
        p = psutil.Process(os.getpid())
        if os.name == 'nt':
            p.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)
        else:
            p.nice(10)
        print('[ä¼˜åŒ–] å·²é™ä½è¿›ç¨‹ä¼˜å…ˆçº§ï¼Œå‡å°‘å¯¹å‰å°ä»»åŠ¡çš„å½±å“')
    except Exception as e:
        print(f'[ä¼˜åŒ–] é™ä½ä¼˜å…ˆçº§å¤±è´¥ï¼ˆä¸å½±å“è¿è¡Œï¼‰: {e}')


def _open_video_capture(video_path, use_gpu=True):
    """
    æ‰“å¼€è§†é¢‘æ–‡ä»¶ã€‚
    å½“ use_gpu=True æ—¶ä¼˜å…ˆä½¿ç”¨ GPU ç¡¬ä»¶åŠ é€Ÿè§£ç ï¼Œä¸å¯ç”¨åˆ™è‡ªåŠ¨å›é€€åˆ° CPUã€‚
    å½“ use_gpu=False æ—¶ç›´æ¥ä½¿ç”¨ CPU è½¯è§£ã€‚
    """
    if use_gpu:
        try:
            params = [cv2.CAP_PROP_HW_ACCELERATION, cv2.VIDEO_ACCELERATION_ANY]
            cap = cv2.VideoCapture(video_path, cv2.CAP_ANY, params)
            if cap.isOpened():
                hw_accel = int(cap.get(cv2.CAP_PROP_HW_ACCELERATION))
                if hw_accel != 0:
                    print(f'[GPU] å·²å¯ç”¨ç¡¬ä»¶åŠ é€Ÿè§£ç  (type={hw_accel})')
                else:
                    print('[GPU] ç¡¬ä»¶åŠ é€Ÿæœªç”Ÿæ•ˆï¼ˆå½“å‰ GPU å¯èƒ½ä¸æ”¯æŒè¯¥ç¼–ç çš„ç¡¬ä»¶è§£ç ï¼‰ï¼Œä½¿ç”¨ CPU è§£ç ')
                return cap
        except (AttributeError, cv2.error) as e:
            print(f'[GPU] ç¡¬ä»¶åŠ é€Ÿä¸å¯ç”¨ ({e})ï¼Œå›é€€åˆ° CPU è§£ç ')
    else:
        print('[CPU] ç”¨æˆ·é€‰æ‹© CPU è§£ç æ¨¡å¼')

    # å›é€€ / CPU æ¨¡å¼: çº¯ CPU è§£ç 
    cap = cv2.VideoCapture(video_path)
    return cap


def extract_slides(video_path, output_dir, threshold=5.0, enable_history=False,
                   max_history=5, use_roi=True, fast_mode=True, use_gpu=True,
                   speed_mode='eco', classroom_mode='ppt',
                   on_progress=None, should_cancel=None,
                   start_frame=0, saved_offset=0):
    """
    ä»è§†é¢‘ä¸­æå–å¹»ç¯ç‰‡æˆªå›¾ã€‚

    Args:
        video_path:      è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_dir:      è¾“å‡ºç›®å½•
        threshold:       åœºæ™¯æ£€æµ‹çµæ•åº¦é˜ˆå€¼
        enable_history:  æ˜¯å¦å¯ç”¨å†å²è®°å¿†æ± 
        max_history:     å†å²è®°å¿†æ± å®¹é‡
        use_roi:         æ˜¯å¦è£å‰ª PPT åŒºåŸŸ
        fast_mode:       æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆé™ä½æ¯”è¾ƒåˆ†è¾¨ç‡ï¼‰
        use_gpu:         æ˜¯å¦ä½¿ç”¨ GPU ç¡¬ä»¶åŠ é€Ÿè§£ç 
        speed_mode:      è¿è¡Œæ¨¡å¼ 'eco'(åå°é™é»˜) | 'fast'(å…¨é€Ÿç‹‚é£™) | 'turbo'(æé€Ÿç‹‚æš´)
        classroom_mode:  è§†é¢‘ç±»å‹ 'ppt'(PPTå½•å±) | 'hybrid'(ç”µå­è¯¾å ‚) | 'blackboard'(å®ä½“è¯¾å ‚)
        on_progress:     è¿›åº¦å›è°ƒ (saved_count, progress_pct, message, eta_seconds, elapsed_seconds[, current_frame])
        should_cancel:   å–æ¶ˆæ£€æŸ¥å›è°ƒ () -> bool
        start_frame:     æ–­ç‚¹ç»­ä¼ ï¼šä»ç¬¬å‡ å¸§å¼€å§‹ï¼ˆ0=ä»å¤´ï¼‰
        saved_offset:    æ–­ç‚¹ç»­ä¼ ï¼šå·²æœ‰å›¾ç‰‡æ•°é‡ï¼ˆæ–‡ä»¶å‘½ååç§»ï¼‰

    Returns:
        (status, message, saved_count) å…ƒç»„
        status: 'done' | 'cancelled' | 'error'
    """
    if on_progress is None:
        on_progress = lambda *args, **kwargs: None
    if should_cancel is None:
        should_cancel = lambda: False

    # â”€â”€ ä¸‰æ¨¡å¼å†…éƒ¨æ ‡å¿— â”€â”€
    if classroom_mode not in ('ppt', 'blackboard', 'hybrid'):
        classroom_mode = 'ppt'
    _use_mog2 = classroom_mode in ('blackboard', 'hybrid')    # éœ€è¦ MOG2 äººç‰©é®ç½©
    _skip_stable = classroom_mode in ('blackboard', 'hybrid')  # è·³è¿‡ç¨³å®šå¸§æ£€æµ‹
    _is_blackboard = (classroom_mode == 'blackboard')          # çº¯é»‘æ¿ç‰¹æœ‰é€»è¾‘
    _mode_label = {'ppt': 'PPT å½•å±', 'hybrid': 'ç”µå­è¯¾å ‚', 'blackboard': 'å®ä½“è¯¾å ‚'}[classroom_mode]

    cap = None
    history_pool = None
    saved = 0

    try:
        # â”€â”€ æ ¹æ®è¿è¡Œæ¨¡å¼é…ç½®èŠ‚æµå’Œä¼˜å…ˆçº§ â”€â”€
        _is_turbo = (speed_mode == 'turbo')
        _is_fast = (speed_mode == 'fast') or _is_turbo
        if _is_fast:
            _THROTTLE_INTERVAL = 0.001  # 1ms å¾®å°é—´éš™ï¼Œä»…è®©å‡º GIL
            if _is_turbo:
                print('[Turbo] æé€Ÿç‹‚æš´æ¨¡å¼ï¼š2xå¸§è·³è· + 320på¯¹æ¯” + åŠ é€Ÿç¨³å®šå¸§æ£€æµ‹')
            else:
                print('[Fast] å…¨é€Ÿç‹‚é£™æ¨¡å¼ï¼šä¿æŒæ­£å¸¸ä¼˜å…ˆçº§ï¼Œæœ€å°èŠ‚æµ')
        else:
            _THROTTLE_INTERVAL = 0.008  # 8ms èŠ‚æµï¼Œé™ä½å³°å€¼å ç”¨
            _lower_process_priority()

        _GC_EVERY_N_FRAMES = 500  # æ¯ 500 å¸§å¼ºåˆ¶ gc.collect() é˜² OOM

        # â”€â”€ ä½¿ç”¨ GPU ç¡¬ä»¶åŠ é€Ÿæ‰“å¼€è§†é¢‘ â”€â”€
        cap = _open_video_capture(video_path, use_gpu=use_gpu)

        # â”€â”€ MOG2 èƒŒæ™¯å»ºæ¨¡ï¼ˆç”µå­è¯¾å ‚ + å®ä½“è¯¾å ‚å…±ç”¨ï¼Œå¿½ç•¥èµ°åŠ¨äººç‰©ï¼‰ â”€â”€
        backSub = None
        _close_kernel = None
        _dilate_kernel = None
        if _use_mog2:
            backSub = cv2.createBackgroundSubtractorMOG2(
                history=500, varThreshold=16, detectShadows=False)
            _close_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))
            _dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (13, 13))
            print(f'[{_mode_label}] MOG2 èƒŒæ™¯å»ºæ¨¡å·²å¯ç”¨ï¼Œå°†å¿½ç•¥ç§»åŠ¨å‰æ™¯')

        total_frames = max(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), 1)
        fps = cap.get(cv2.CAP_PROP_FPS) or 30
        # é»‘æ¿æ¨¡å¼æ­¥é•¿ï¼ˆä»… PyAV ä¸å¯ç”¨æ—¶çš„ OpenCV å›é€€ï¼‰ï¼š10 ç§’
        # æ­¥é•¿è®¾ç½®ï¼ˆPyAV NONKEY å¯ç”¨æ—¶ä¼šåœ¨åé¢è¦†ç›–ï¼‰
        if _is_blackboard:
            frame_step = max(1, int(fps * 10))   # æ¿ä¹¦æ¸å˜ï¼Œå¤§æ­¥é•¿
        elif classroom_mode == 'hybrid':
            frame_step = max(1, int(fps * 3))    # ç”µå­è¯¾å ‚ï¼ŒçŸ­æ­¥é•¿æŠ“ç¿»é¡µ
        else:
            frame_step = max(1, int(fps * (2 if _is_turbo else 1)))  # PPT å½•å±

        # â”€â”€ æ–­ç‚¹ç»­ä¼ ï¼šè·³åˆ°ä¸Šæ¬¡ä¸­æ–­çš„ä½ç½® â”€â”€
        is_resuming = (start_frame > 0)
        if is_resuming:
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            print(f'[æ–­ç‚¹ç»­ä¼ ] ä»ç¬¬ {start_frame} å¸§æ¢å¤ï¼Œå·²æœ‰ {saved_offset} å¼ å›¾ç‰‡')

        ok, prev_frame = cap.read()
        if not ok:
            return ('error', 'æ— æ³•è¯»å–è§†é¢‘æ–‡ä»¶', 0)

        count = start_frame if is_resuming else 0

        h, w = prev_frame.shape[:2]
        if use_roi:
            y1, y2 = int(h * 0.185), h
            x1, x2 = int(w * 0.208), w
        else:
            y1, y2 = 0, h
            x1, x2 = 0, w

        roi_w = x2 - x1
        # Turbo: 320p è¶…ä½åˆ†è¾¨ç‡å¯¹æ¯”ï¼ˆåƒç´ å‡ 55%ï¼‰; Fast/Eco: 480p
        COMPARE_WIDTH = 320 if _is_turbo else 480
        if fast_mode and roi_w > COMPARE_WIDTH:
            _scale = COMPARE_WIDTH / roi_w
        else:
            _scale = 1.0

        def _to_gray(frame):
            roi = frame[y1:y2, x1:x2]
            if _scale < 1.0:
                roi = cv2.resize(roi, None, fx=_scale, fy=_scale,
                                 interpolation=cv2.INTER_AREA)
            return cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

        prev_gray = _to_gray(prev_frame)
        if backSub is not None:
            backSub.apply(prev_gray)  # é¦–å¸§å–‚å…¥ MOG2 å¼€å§‹å»ºæ¨¡
            prev_bg_mask = np.ones_like(prev_gray, dtype=np.uint8) * 255  # é¦–å¸§æ— å‰æ™¯å†å²
        history_pool = [prev_gray] if enable_history else None

        # â”€â”€ æ€§èƒ½ä¼˜åŒ–ï¼šJPEG è´¨é‡ / seek è·³è½¬ / å¼‚æ­¥ä¿å­˜ â”€â”€
        _JPEG_QUALITY = 85 if _is_blackboard else 95
        _USE_SEEK = (backSub is not None)  # ç”µå­è¯¾å ‚/å®ä½“è¯¾å ‚å¯ç”¨ seek è·³è½¬

        # â”€â”€ PyAV åŠ é€Ÿï¼šä»…è§£ç å…³é”®å¸§ï¼ˆskip_frame=NONKEYï¼‰ â”€â”€
        # å¯¹æ‰€æœ‰æ¨¡å¼ç”Ÿæ•ˆï¼šPPT æ¨¡å¼åŒæ ·å—ç›Šï¼ˆAV1 é¡ºåº grab ææ…¢ï¼‰
        # use_gpu=True æ—¶ä½¿ç”¨å¯åŠ¨æ—¶ç¼“å­˜çš„æ¢æµ‹ç»“æœï¼Œç›´æ¥é€‰ç”¨æœ€ä¼˜ hw_type
        _av_container = None
        _av_stream = None
        _keyframe_iter = None
        if HAS_PYAV:
            _pyav_hw = ''
            _codec_name = ''
            # æ¢æµ‹è§†é¢‘ç¼–ç æ ¼å¼
            try:
                _probe = av.open(video_path)
                _codec_name = _probe.streams.video[0].codec_context.name
                _probe.close()
            except Exception:
                pass

            # ä½¿ç”¨å¯åŠ¨æ—¶ç¼“å­˜çš„æ¢æµ‹ç»“æœï¼Œä»…å°è¯•å·²çŸ¥å¯ç”¨çš„ hw_type
            if use_gpu and _codec_name:
                _cached = probe_gpu()
                _best_hw = _cached.get('best_per_codec', {}).get(_codec_name)
                if _best_hw:
                    try:
                        from av.codec.hwaccel import HWAccel
                        _hwaccel = HWAccel(codec=_codec_name, device_type=_best_hw)
                        _av_container = av.open(video_path, hwaccel=_hwaccel)
                        _av_stream = _av_container.streams.video[0]
                        _av_stream.thread_type = 'AUTO'
                        _av_stream.codec_context.skip_frame = 'NONKEY'
                        _keyframe_iter = _av_container.decode(_av_stream)
                        # è¯•è§£ä¸€å¸§ç¡®è®¤ç¡¬ä»¶è§£ç ç¡®å®å¯ç”¨
                        next(_keyframe_iter).to_ndarray(format='bgr24')
                        _pyav_hw = _best_hw
                    except Exception:
                        if _av_container is not None:
                            try: _av_container.close()
                            except Exception: pass
                            _av_container = None
                        _keyframe_iter = None

            # è½¯ä»¶è§£ç å›é€€ï¼ˆdav1d è§£ AV1 ä»æå¿«ï¼‰
            if _keyframe_iter is None:
                try:
                    _av_container = av.open(video_path)
                    _av_stream = _av_container.streams.video[0]
                    _av_stream.thread_type = 'AUTO'
                    _av_stream.codec_context.skip_frame = 'NONKEY'
                    if not _codec_name:
                        _codec_name = _av_stream.codec_context.name
                    _keyframe_iter = _av_container.decode(_av_stream)
                except Exception as e:
                    print(f'[PyAV] åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€ OpenCV: {e}')
                    if _av_container is not None:
                        try: _av_container.close()
                        except Exception: pass
                    _av_container = None
                    _keyframe_iter = None

            if _keyframe_iter is not None:
                _hw_label = f'GPU {_pyav_hw}' if _pyav_hw else 'CPU dav1d'
                print(f'[PyAV] æ£€æµ‹åˆ° {_codec_name}ï¼Œå¯ç”¨å…³é”®å¸§å¿«é€Ÿè¿­ä»£ï¼ˆskip_frame=NONKEYï¼Œ{_hw_label}ï¼‰')

                # NONKEY + é»‘æ¿æ¨¡å¼ï¼šä¸¤éæ‰«æç­–ç•¥
                # ç¬¬ä¸€éï¼ˆé¢„è®­ç»ƒï¼‰ï¼šå¿«é€Ÿæ‰«å®Œå…¨éƒ¨å…³é”®å¸§ï¼Œåªå–‚ MOG2ï¼Œä¸åšæ¯”è¾ƒ
                # ç¬¬äºŒéï¼ˆæå–ï¼‰ï¼š  seek å›å¼€å¤´ï¼Œç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åšç²¾ç¡®é®ç½©
                if backSub is not None:
                    backSub.setHistory(60)
                    print(f'[{_mode_label}] MOG2 é¢„è®­ç»ƒï¼šæ‰«æå…¨éƒ¨å…³é”®å¸§å»ºç«‹èƒŒæ™¯æ¨¡å‹â€¦')
                    _warmup_count = 0
                    try:
                        for _wf in _keyframe_iter:
                            _wg = _to_gray(_wf.to_ndarray(format='bgr24'))
                            backSub.apply(_wg, learningRate=0.02)
                            _warmup_count += 1
                    except StopIteration:
                        pass
                    print(f'[{_mode_label}] MOG2 é¢„è®­ç»ƒå®Œæˆï¼šå·²å­¦ä¹  {_warmup_count} ä¸ªå…³é”®å¸§')
                    # seek å›èµ·ç‚¹ï¼Œé‡å»ºå…³é”®å¸§è¿­ä»£å™¨
                    _av_container.seek(0)
                    _keyframe_iter = _av_container.decode(_av_stream)
                    # NONKEY æ­¥é•¿è¦†ç›–ï¼šå®ä½“è¯¾å ‚ 5 ç§’ï¼Œç”µå­è¯¾å ‚ 3 ç§’
                    if _is_blackboard:
                        frame_step = max(1, int(fps * 5))
                    else:
                        frame_step = max(1, int(fps * 3))

        def _advance(frames_to_skip):
            """è·³è¿‡æŒ‡å®šå¸§æ•°ã€‚ä¼˜å…ˆç”¨ PyAV å…³é”®å¸§è¿­ä»£ï¼ˆæ‰€æœ‰æ¨¡å¼ï¼‰ï¼Œå¤±è´¥åˆ™å›é€€ seek/grabã€‚"""
            nonlocal count, _keyframe_iter
            if frames_to_skip <= 0:
                return True, None

            # PyAV NONKEY æ¨¡å¼ï¼šè·å–ä¸‹ä¸€ä¸ªæ»¡è¶³é—´è·çš„å…³é”®å¸§
            # å¯¹å¯†é›†å…³é”®å¸§è§†é¢‘ï¼ˆå¦‚ H.264 æ¯ç§’ä¸€ä¸ª I å¸§ï¼‰ï¼Œè·³è¿‡é—´è·ä¸è¶³çš„å…³é”®å¸§
            if _keyframe_iter is not None:
                try:
                    target_count = count + frames_to_skip
                    while True:
                        frame = next(_keyframe_iter)
                        if frame.pts is not None and _av_stream.time_base:
                            actual_sec = float(frame.pts * _av_stream.time_base)
                            frame_count = int(actual_sec * fps)
                        else:
                            frame_count = target_count  # æ—  PTS æ—¶ç›´æ¥ä½¿ç”¨
                        if frame_count >= target_count:
                            arr = frame.to_ndarray(format='bgr24')
                            count = frame_count
                            return True, arr
                        # æ­¤å…³é”®å¸§ç¦»ä¸Šä¸€å¸§å¤ªè¿‘ï¼Œè·³è¿‡ï¼ˆä¸åš to_ndarray çœå¼€é”€ï¼‰
                except StopIteration:
                    return False, None
                except Exception as e:
                    print(f'[PyAV] å…³é”®å¸§è¿­ä»£å¤±è´¥ ({e})ï¼Œå›é€€ OpenCV')
                    _keyframe_iter = None  # åç»­ä¸å†å°è¯•

            # OpenCV seekï¼ˆé»‘æ¿æ¨¡å¼å¤‡é€‰ï¼‰
            if _USE_SEEK:
                target_frame = count + frames_to_skip
                seek_ok = cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                if seek_ok:
                    ok, frame = cap.read()
                    if ok:
                        count = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
                        return ok, frame
                # seek å¤±è´¥ï¼ˆMSMF åç«¯é™åˆ¶ï¼‰ï¼Œå›é€€é¡ºåº grab
                print(f'[Blackboard] seek å›é€€ä¸ºé¡ºåº grabï¼ˆtarget={target_frame}ï¼‰')
            # PPT æ¨¡å¼ / seek å›é€€ï¼šé¡ºåº grab
            for _ in range(frames_to_skip):
                count += 1
                if not cap.grab():
                    return False, None
            ok, frame = cap.retrieve()
            return ok, frame

        _save_pool = ThreadPoolExecutor(max_workers=2)
        _save_futures = []

        def _async_save(frame, filepath, quality):
            buf = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, quality])[1]
            buf.tofile(filepath)

        _extract_start_time = time.time()

        # â”€â”€ ä¿å­˜ç¬¬ä¸€å¸§ï¼ˆç»­ä¼ æ—¶è·³è¿‡ï¼Œå› ä¸ºæ–­ç‚¹å¸§åªç”¨äºæ¯”è¾ƒåŸºå‡†ï¼‰ â”€â”€
        if not is_resuming:
            fp = os.path.join(output_dir, f"slide_{saved_offset + saved:04d}.jpg")
            _save_futures.append(_save_pool.submit(_async_save, prev_frame.copy(), fp, _JPEG_QUALITY))
            saved += 1
            on_progress(saved, 0, f'å·²æå– {saved_offset + saved} å¼ ', -1, 0, count)
        else:
            on_progress(saved, int(count / total_frames * 100),
                        f'ä»æ–­ç‚¹æ¢å¤ï¼Œç»§ç»­æå–â€¦', -1, 0, count)

        while True:
            if should_cancel():
                return ('cancelled', f'å·²å–æ¶ˆï¼Œå·²ä¿å­˜ {saved_offset + saved} å¼ ', saved)

            # â”€â”€ èŠ‚æµï¼šè®©å‡ºå°‘é‡ CPU ç»™ç³»ç»Ÿå’Œå…¶ä»–çº¿ç¨‹ â”€â”€
            time.sleep(_THROTTLE_INTERVAL)

            ok, curr_frame = _advance(frame_step)
            if not ok or curr_frame is None:
                break

            # â”€â”€ å®šæœŸ gc é˜²æ­¢å†…å­˜æº¢å‡ºï¼ˆFast æ¨¡å¼ä¸‹äº§ç”Ÿå¸§æ•°ç»„æå¿«ï¼‰ â”€â”€
            if count % _GC_EVERY_N_FRAMES == 0:
                gc.collect()

            if should_cancel():
                return ('cancelled', f'å·²å–æ¶ˆï¼Œå·²ä¿å­˜ {saved_offset + saved} å¼ ', saved)

            pct = min(99, int(count / total_frames * 100))
            elapsed = time.time() - _extract_start_time
            if pct > 2:
                eta = elapsed / pct * (100 - pct)
            else:
                eta = -1
            on_progress(saved, pct, f'å·²æå– {saved_offset + saved} å¼ ', round(eta, 1), round(elapsed, 1), count)

            curr_gray = _to_gray(curr_frame)

            # â”€â”€ è®¡ç®—å¸§é—´å·®å¼‚ï¼ˆå®ä½“è¯¾å ‚æ¨¡å¼ï¼šäº¤é›†æ©ç æ¶ˆé™¤æ®‹å½±ï¼‰ â”€â”€
            if backSub is not None:
                _bb_lr = 0.005 if _keyframe_iter is not None else -1
                fg_mask = backSub.apply(curr_gray, learningRate=_bb_lr)
                # å½¢æ€å­¦å¤„ç†ï¼šå…ˆé—­åˆå¡«å……äººç‰©è½®å»“å†…ç©ºæ´ï¼Œå†è†¨èƒ€æ‰©å¤§é®ç½©è¦†ç›–èŒƒå›´
                fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, _close_kernel)
                fg_mask = cv2.dilate(fg_mask, _dilate_kernel, iterations=2)
                bg_mask = cv2.bitwise_not(fg_mask)
                # äº¤é›†æ©ç ï¼šåŒæ—¶æ’é™¤äººç‰©"ç°åœ¨çš„ä½ç½®"å’Œ"åˆšæ‰çš„ä½ç½®"
                combined_bg = cv2.bitwise_and(bg_mask, prev_bg_mask)
                valid_pixels = cv2.countNonZero(combined_bg)
                total_pixels = curr_gray.shape[0] * curr_gray.shape[1]
                if valid_pixels < total_pixels * 0.10:
                    mean_diff = 0  # äººæŒ¡ä½äº†å¤§éƒ¨åˆ†ç”»é¢ï¼Œè·³è¿‡
                else:
                    diff = cv2.absdiff(curr_gray, prev_gray)
                    masked_diff = cv2.bitwise_and(diff, diff, mask=combined_bg)
                    mean_diff = np.sum(masked_diff) / valid_pixels
            else:
                mean_diff = np.mean(cv2.absdiff(curr_gray, prev_gray))

            if mean_diff > threshold:
                if _skip_stable:
                    # â”€â”€ ç”µå­è¯¾å ‚ / å®ä½“è¯¾å ‚ï¼šç›´æ¥æˆªå›¾ï¼Œä¸ç­‰ç¨³å®š â”€â”€
                    settled_frame = curr_frame
                    settled_gray = curr_gray
                elif _keyframe_iter is not None:
                    # â”€â”€ PPT + NONKEYï¼šç”¨åç»­å…³é”®å¸§åšç¨³å®šæ£€æµ‹ï¼ˆç­‰ PPT åŠ¨ç”»æ’­å®Œï¼‰ â”€â”€
                    _stable_need = 1 if _is_turbo else 2
                    stable = 0
                    last_gray = curr_gray
                    settled_frame = None
                    settled_gray = None
                    for _ in range(10):  # æœ€å¤šæ£€æŸ¥ 10 ä¸ªåç»­å…³é”®å¸§
                        if should_cancel():
                            break
                        time.sleep(_THROTTLE_INTERVAL)
                        try:
                            sf = next(_keyframe_iter)
                            if sf.pts is not None and _av_stream.time_base:
                                count = int(float(sf.pts * _av_stream.time_base) * fps)
                            tmp_frame = sf.to_ndarray(format='bgr24')
                            tmp_gray = _to_gray(tmp_frame)
                            if np.mean(cv2.absdiff(tmp_gray, last_gray)) < max(threshold * 0.4, 2.5):
                                stable += 1
                            else:
                                stable = 0
                            last_gray = tmp_gray
                            if stable >= _stable_need:
                                settled_frame = tmp_frame
                                settled_gray = tmp_gray
                                break
                        except StopIteration:
                            break
                else:
                    # â”€â”€ PPT æ¨¡å¼ï¼šç¨³å®šå¸§æ£€æµ‹ï¼ˆç­‰åŠ¨ç”»æ’­å®Œå†æˆªå›¾ï¼‰ â”€â”€
                    _stable_secs = 0.3 if _is_turbo else 0.5
                    _stable_need = 1 if _is_turbo else 2
                    check_step = max(1, int(fps * _stable_secs))
                    stable = 0
                    last_gray = curr_gray
                    settled_frame = None
                    settled_gray = None

                    while True:
                        if should_cancel():
                            break
                        time.sleep(_THROTTLE_INTERVAL)
                        s_grabbed = True
                        for _ in range(check_step):
                            count += 1
                            if not cap.grab():
                                s_grabbed = False
                                break
                        if not s_grabbed:
                            break
                        ret, tmp = cap.retrieve()
                        if not ret:
                            break
                        tmp_gray = _to_gray(tmp)
                        if np.mean(cv2.absdiff(tmp_gray, last_gray)) < 1.0:
                            stable += 1
                        else:
                            stable = 0
                        last_gray = tmp_gray
                        if stable >= _stable_need:
                            settled_frame = tmp
                            settled_gray = tmp_gray
                            break

                # ç¨³å®šå¸§æ£€æµ‹åå†æ£€æŸ¥ä¸€æ¬¡å–æ¶ˆ
                if should_cancel():
                    return ('cancelled', f'å·²å–æ¶ˆï¼Œå·²ä¿å­˜ {saved_offset + saved} å¼ ', saved)

                if settled_gray is not None:
                    final_diff = np.mean(cv2.absdiff(settled_gray, prev_gray))
                    dup = False
                    if enable_history and history_pool:
                        for pg in history_pool:
                            if np.mean(cv2.absdiff(settled_gray, pg)) <= threshold:
                                dup = True
                                break
                    elif final_diff <= threshold:
                        dup = True

                    if not dup and final_diff > threshold:
                        fp = os.path.join(output_dir, f"slide_{saved_offset + saved:04d}.jpg")
                        _save_futures.append(_save_pool.submit(_async_save, settled_frame.copy(), fp, _JPEG_QUALITY))
                        saved += 1
                        on_progress(saved, pct, f'å·²æå– {saved_offset + saved} å¼ ',
                                    round(eta, 1), round(elapsed, 1), count)
                        prev_gray = settled_gray
                        if backSub is not None:
                            prev_bg_mask = bg_mask.copy()
                            # 15 ç§’æ­¥é•¿æœ¬èº«å·²æä¾›è¶³å¤Ÿé—´éš”ï¼Œæ— éœ€é¢å¤–å†·å´
                        if enable_history:
                            history_pool.append(settled_gray)
                            if len(history_pool) > max_history:
                                history_pool.pop(0)
                    else:
                        prev_gray = settled_gray
                        if backSub is not None:
                            prev_bg_mask = bg_mask.copy()

        # â”€â”€ å°¾å¸§ä¿æŠ¤ï¼šæ•è·è§†é¢‘æœ€åä¸€å¸§çš„æ¿ä¹¦çŠ¶æ€ â”€â”€
        # ä¸»å¾ªç¯å›  _advance() åˆ°è¾¾è§†é¢‘æœ«å°¾è€Œ breakï¼Œæœ€åä¸€æ®µæ¿ä¹¦å¯èƒ½è¢«è·³è¿‡
        if backSub is not None and not should_cancel():
            cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)
            ok_last, last_frame = cap.read()
            if ok_last and last_frame is not None:
                last_gray = _to_gray(last_frame)
                fg_mask = backSub.apply(last_gray)
                fg_mask = cv2.dilate(fg_mask, None, iterations=2)
                bg_mask = cv2.bitwise_not(fg_mask)
                combined_bg = cv2.bitwise_and(bg_mask, prev_bg_mask)
                valid_pixels = cv2.countNonZero(combined_bg)
                total_pixels = last_gray.shape[0] * last_gray.shape[1]
                if valid_pixels >= total_pixels * 0.10:
                    diff = cv2.absdiff(last_gray, prev_gray)
                    masked_diff = cv2.bitwise_and(diff, diff, mask=combined_bg)
                    last_diff = np.sum(masked_diff) / valid_pixels
                    if last_diff > threshold:
                        fp = os.path.join(output_dir, f"slide_{saved_offset + saved:04d}.jpg")
                        _save_futures.append(_save_pool.submit(_async_save, last_frame.copy(), fp, _JPEG_QUALITY))
                        saved += 1
                        print(f'[Blackboard] å°¾å¸§ä¿æŠ¤ï¼šæ•è·æœ€åä¸€å¸§æ¿ä¹¦ï¼ˆdiff={last_diff:.1f}ï¼‰')

        elapsed_total = round(time.time() - _extract_start_time, 1)
        total_saved = saved_offset + saved
        return ('done',
                f'æå–å®Œæˆï¼å…± {total_saved} å¼ å¹»ç¯ç‰‡ï¼Œè€—æ—¶ {int(elapsed_total)}s',
                saved)

    except Exception as e:
        error_detail = traceback.format_exc()
        print(f"ï¼ï¼ï¼ æå–å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼ï¼ï¼\n{error_detail}")
        err_msg = str(e)
        if 'memory' in err_msg.lower() or 'MemoryError' in type(e).__name__:
            hint = 'å†…å­˜ä¸è¶³ï¼Œè¯·å…³é—­å…¶ä»–æ ‡ç­¾é¡µæˆ–ç¨‹åºåé‡è¯•ã€‚'
        elif 'permission' in err_msg.lower() or 'access' in err_msg.lower():
            hint = 'æ–‡ä»¶æƒé™è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ­£åœ¨è¢«å…¶ä»–ç¨‹åºä½¿ç”¨ã€‚'
        elif isinstance(e, cv2.error):
            hint = 'è§†é¢‘å¤„ç†å‡ºé”™ï¼Œå»ºè®®ç”¨ FFmpeg è½¬ç åé‡è¯•ã€‚'
        else:
            hint = 'è¯·æˆªå›¾æ­¤é”™è¯¯å¹¶å‰å¾€ GitHub Issues åé¦ˆã€‚'
        return ('error', f'æå–å‡ºé”™: {err_msg}\nğŸ’¡ {hint}', saved)

    finally:
        # â”€â”€ ç­‰å¾…æ‰€æœ‰å¼‚æ­¥ä¿å­˜å®Œæˆ â”€â”€
        for f in _save_futures:
            try:
                f.result()
            except Exception as save_err:
                print(f'[ä¿å­˜] å¼‚æ­¥å†™ç›˜å¤±è´¥: {save_err}')
        try:
            _save_pool.shutdown(wait=False)
        except Exception:
            pass
        # â”€â”€ å…³é—­ PyAV èµ„æº â”€â”€
        if _av_container is not None:
            try:
                _av_container.close()
            except Exception:
                pass
        # â”€â”€ ç¡®ä¿é‡Šæ”¾æ‰€æœ‰é‡é‡çº§èµ„æº â”€â”€
        if cap is not None:
            try:
                cap.release()
            except Exception:
                pass
        cap = None
        history_pool = None
        # ç«‹å³è§¦å‘åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å¤§é‡ numpy æ•°ç»„å ç”¨çš„å†…å­˜
        gc.collect()
